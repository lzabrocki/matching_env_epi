---
title: "Propensity Score Matching"
description: |
  Detailled Script.
author:
  - name: Tarik Benmarhnia
    url: https://profiles.ucsd.edu/tarik.benmarhnia
    affiliation: UCSD & Scripps Institute
    affiliation_url: https://benmarhniaresearch.ucsd.edu/
  - name: Marie-Abèle Bind 
    url: https://scholar.harvard.edu/marie-abele
    affiliation: Biostatistics Center, Massachusetts General Hospital
    affiliation_url: https://biostatistics.massgeneral.org/faculty/marie-abele-bind-phd/
  - name: Léo Zabrocki 
    url: https://lzabrocki.github.io/
    affiliation: Paris School of Economics
    affiliation_url: https://www.parisschoolofeconomics.eu/fr/zabrocki-leo/
date: "`r Sys.Date()`"
output: 
    distill::distill_article:
      keep_md: true
      toc: true
      toc_depth: 3
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
# code chunk option
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  layout = "l-body-outset",
  dev = "CairoPNG",
  dpi = 400
)
```

<style>
body {
text-align: justify}
</style>

In this document, we provide all steps and R codes required to estimate the effect of heat waves of the number of years of life lost (YoLL) using propensity score matching. The implementation is done with the fantastic package [MatchIt](https://kosukeimai.github.io/MatchIt/index.html): do not hesitate to explore its very well-made documentation. We also rely on the [cobalt](https://cran.r-project.org/web/packages/cobalt/vignettes/cobalt.html) for checking covariate balance. **Should you have any questions, need help to reproduce the analysis or find coding errors, please do not hesitate to contact us at leo.zabrocki@psemail.eu**

# Required Packages and Data Loading

To reproduce exactly the `4_propensity_score_matching.html` document, we first need to have installed:

* the [R](https://www.r-project.org/) programming language 
* [RStudio](https://rstudio.com/), an integrated development environment for R, which will allow you to knit the `4_propensity_score_matching.Rmd` file and interact with the R code chunks
* the [R Markdown](https://rmarkdown.rstudio.com/) package
* and the [Distill](https://rstudio.github.io/distill/) package which provides the template for this document. 

Once everything is set up, we load the following packages:

```{r}
# load required packages
library(knitr) # for creating the R Markdown document
library(here) # for files paths organization
library(tidyverse) # for data manipulation and visualization
library(broom) # for cleaning regression outputs
library(MatchIt) # for matching procedures
library(cobalt) # for assessing covariates balance
library(lmtest) # for modifying regression standard errors
library(sandwich) # for robust and cluster robust standard errors
library(Cairo) # for printing custom police of graphs
library(DT) # for displaying the data as tables
```

We load our custom `ggplot2` theme for graphs:

```{r}
# load ggplot custom theme
source(here::here("2.scripts",
                  "functions",
                  "script_theme_tufte.R"))
# define nice colors
my_blue <- "#0081a7"
my_orange <- "#fb8500"
```

We finally load the data:

```{r}
# load the data
data <-
  readRDS(here::here("1.data", "simulated_environmental_data.rds")) %>%
  # define week and year as factors
  mutate_at(vars(week, year), ~ as.factor(.))
```

As a reminder, there are `r nrow(data %>% filter(heat_wave==1))` days where an heat wave occurred and `r nrow(data %>% filter(heat_wave==0))` days without heat waves.

# Propensity Score Matching

We implement below a propensity score matching procedure where:

* each day with an heat wave is matched to the most similar day without heat wave. This is a 1:1 nearest neighbor matching without replacement.
* the distance metric used for the matching is the propensity score which is predicted using a logistic model where we regress the heat wave dummy on its three lags, the three lags of ozone, nitrogen dioxide and its three lags, the relative humidity, the calendar indicators for the day of the week, the week and the year.

We vary the matching distance to see how covariates balance change:

1. We first match each treated unit to its closest control unit using a caliper equal to a 1 standard deviation of the estimated propensity scores.
2. We then set the maximum distance to be inferior to 0.5 propensity score standard deviation. 

Once treated and control units are matched, we assess whether covariates balance has improved. 

We finally estimate the treatment effect.

### Matching Procedure and Covariates Balance Improvement

We first match each treated unit to its closest control unit with a 1 standard deviation caliper using the `matchit()` function:

```{r}
# match without caliper
matching_ps_1_caliper <-
  matchit(
    heat_wave ~ heat_wave_lag_1 + heat_wave_lag_2 + heat_wave_lag_3 +
      o3_lag_1 + o3_lag_2 + o3_lag_3 +
      no2 + no2_lag_1 + no2_lag_2 + no2_lag_3 +
      humidity_relative + month + week + year,
    caliper = 1,
    data = data
  )

# display summary of the procedure
matching_ps_1_caliper
```

The output of the matching procedure indicates us the method (1:1 nearest neighbor matching without replacement) and the distance (propensity score) we used. It also tells us how many treated units were matched: `r matching_ps_1_caliper[["nn"]][3]` (2 treated units were not matched). We assess how covariates balance has improved by comparing the distribution of propensity scores before and after matching: 

```{r, fig.width = 10, fig.height = 6, code_folding="Please show me the code!"}
# distribution of propensity scores
graph_propensity_score_distribution_1 <- bal.plot(
  matching_ps_1_caliper,
  var.name = "distance",
  which = "both",
  sample.names = c("Initial Data", "Matched Data"),
  type = "density") +
  ggtitle("Distribution of the Propensity Score Before and After Matching") +
  xlab("Propensity Scores") +
  scale_fill_manual(
    name = "Group:",
    values = c(my_blue, my_orange),
    labels = c("Days without Heat Waves", "Days with Heat Waves")
  ) +
  theme_tufte()

# display the graph
graph_propensity_score_distribution_1

# save the graph
ggsave(
  graph_propensity_score_distribution_1 + labs(title = NULL),
  filename = here::here(
    "3.outputs",
    "2.graphs",
    "graph_propensity_score_distribution_1.pdf"
  ),
  width = 16,
  height = 10,
  units = "cm",
  device = cairo_pdf
)
```

We see on this graph that propensity scores distribution for the two groups better overlap after matching. We can also evaluate the covariates balance using the `love.plot()` function from the cobalt package and the absolute mean difference as the summary statistic. For binary variables, the absolute difference in proportion is computed. For continuous covariates, denoted with a star, the absolute standardized mean difference is computed (the difference is divided by the standard deviation of the variable for treated units before matching).

```{r, fig.width = 8, fig.height = 8, code_folding="Please show me the code!"}
# first we nicely label covariates
cov_labels <- c(
  heat_wave_lag_1 = "Heat Wave t-1",
  heat_wave_lag_2 = "Heat Wave t-2",
  heat_wave_lag_3 = "Heat Wave t-3",
  o3_lag_1 = "O3 t-1",
  o3_lag_2 = "O3 t-2",
  o3_lag_3 = "O3 t-3",
  no2 = "NO2",
  no2_lag_1 = "NO2 t-1",
  no2_lag_2 = "NO2 t-2",
  no2_lag_3 = "NO2 t-3",
  humidity_relative = "Relative Humidity",
  month_august = "August",
  month_june = "June",
  month_july = "July",
  week_22 = "Week 22",
  week_23 = "Week 23",
  week_24 = "Week 24",
  week_25 = "Week 25",
  week_26 = "Week 26",
  week_27 = "Week 27",
  week_28 = "Week 28",
  week_29 = "Week 29",
  week_30 = "Week 30",
  week_31 = "Week 31",
  week_32 = "Week 32",
  week_33 = "Week 33",
  week_34 = "Week 34",
  week_35 = "Week 35",
  year_1990 = "1990",
  year_1991 = "1991",
  year_1992 = "1992",
  year_1993 = "1993",
  year_1994 = "1994",
  year_1995 = "1995",
  year_1996 = "1996",
  year_1997 = "1997",
  year_1998 = "1998",
  year_1999 = "1999",
  year_2000 = "2000",
  year_2001 = "2001",
  year_2002 = "2002",
  year_2003 = "2003",
  year_2004 = "2004",
  year_2005 = "2005",
  year_2006 = "2006",
  year_2007 = "2007"
)

# make the love plot
graph_love_plot_ps_1 <- love.plot(
  matching_ps_1_caliper,
  drop.distance = TRUE,
  abs = TRUE,
  var.order = "unadjusted",
  binary = "raw",
  s.d.denom = "treated",
  thresholds = c(m = .1),
  var.names = cov_labels,
  sample.names = c("Initial Data", "Matched Data"),
  shapes = c("circle", "triangle"),
  colors = c(my_orange, my_blue),
  stars = "std"
) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
  xlab("Absolute Mean Differences") +
  theme_tufte()

# display the graph
graph_love_plot_ps_1

# save the graph
ggsave(
  graph_love_plot_ps_1 + labs(title = NULL),
  filename = here::here(
    "3.outputs",
    "2.graphs",
    "graph_love_plot_ps_1.pdf"
  ),
  width = 20,
  height = 15,
  units = "cm",
  device = cairo_pdf
)
```

On this graph, we can see that for most covariates balance has improved after matching---yet, for few covariates, the standardized mean difference has increased. We display below the evolution of the average of standardized mean differences for continuous covariates:

```{r}
graph_love_plot_ps_1[["data"]] %>%
  filter(type == "Contin.") %>%
  group_by(Sample) %>%
  summarise("Average of Standardized Mean Differences" = round(mean(stat), 2),
            "Std. Deviation of Standardized Mean Differences" = round(sd(stat), 2)) %>%
  kable(align = c("l", "c"))
```

We also display below the evolution of the difference in proportions for binary covariates:

```{r}
graph_love_plot_ps_1[["data"]] %>%
  filter(type == "Binary") %>%
  group_by(Sample) %>%
  summarise("Average of Standardized Mean Differences" = round(mean(stat), 2),
            "Std. Deviation of Standardized Mean Differences" = round(sd(stat), 2)) %>%
  kable(align = c("l", "c"))
```

Overall, for both types of covariates, the balance has clearly improved after matching.

Until now, we matched each treated unit to its closest control unit according to 1 standard deviation caliper: we could however make sure that a treated unit is not matched to a control unit which is too much different. We do so by setting a caliper of 0.5 standard deviation:

```{r}
# match without caliper
matching_ps_0.5_caliper <-
  matchit(
    heat_wave ~ heat_wave_lag_1 + heat_wave_lag_2 + heat_wave_lag_3 +
      o3_lag_1 + o3_lag_2 + o3_lag_3 +
      no2 + no2_lag_1 + no2_lag_2 + no2_lag_3 +
      humidity_relative + month + week + year,
    caliper = 0.5,
    data = data
  )

# display summary of the procedure
matching_ps_0.5_caliper
```

Compared to the matching with a 1 standard deivation caliper, there are now `r matching_ps_0.5_caliper[["nn"]][3]` matched treated units. We can check whether the propensity score distributions overlap better:

```{r, fig.width = 10, fig.height = 6, code_folding="Please show me the code!"}
# distribution of propensity scores
graph_propensity_score_distribution_2 <- bal.plot(
  matching_ps_0.5_caliper,
  var.name = "distance",
  which = "both",
  sample.names = c("Initial Data", "Matched Data"),
  type = "density"
) +
  ggtitle("Distribution of the Propensity Score Before and After Matching") +
  xlab("Propensity Scores") +
  scale_fill_manual(
    name = "Group:",
    values = c(my_blue, my_orange),
    labels = c("Days without Heat Waves", "Days with Heat Waves")
  ) +
  theme_tufte()

# display the graph
graph_propensity_score_distribution_2

# save the graph
ggsave(
  graph_propensity_score_distribution_2 + labs(title = NULL),
  filename = here::here(
    "3.outputs",
    "2.graphs",
    "graph_propensity_score_distribution_2.pdf"
  ),
  width = 16,
  height = 10,
  units = "cm",
  device = cairo_pdf
)
```

It seems to be better than the matching without caliper. We can also evaluate how each covariate balance has improved with a love plot:

```{r, fig.width = 8, fig.height = 8}
# make the love plot
graph_love_plot_ps_2 <- love.plot(
  heat_wave ~ heat_wave_lag_1 + heat_wave_lag_2 + heat_wave_lag_3 + o3_lag_1 + o3_lag_2 + o3_lag_3 + no2 + no2_lag_1 + no2_lag_2 + no2_lag_3 + humidity_relative + month + week + year,
  data = data,
  estimand = "ATT",
  weights = list("Without Caliper" = matching_ps_1_caliper,
                 "With Caliper" = matching_ps_0.5_caliper),
  drop.distance = TRUE,
  abs = TRUE,
  var.order = "unadjusted",
  binary = "raw",
  s.d.denom = "treated",
  thresholds = c(m = .1),
  var.names = cov_labels,
  sample.names = c("Initial Data", "Without Caliper", "With Caliper"),
  shapes = c("circle", "triangle", "square"),
  colors = c(my_orange, my_blue, "#81b29a"),
  stars = "std"
) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
  xlab("Absolute Mean Differences") +
  theme_tufte()

# display the graph
graph_love_plot_ps_2

# save the graph
ggsave(
  graph_love_plot_ps_2,
  filename = here::here(
    "3.outputs",
    "2.graphs",
    "graph_love_plot_ps_2.pdf"
  ),
  width = 20,
  height = 15,
  units = "cm",
  device = cairo_pdf
)
```

On this graph, it is not clear to see whether covariates balance has really increased. We display below, for continuous covariates, the average of standardized mean differences for the three datasets:

```{r}
graph_love_plot_ps_2[["data"]] %>%
  filter(type == "Contin.") %>%
  group_by(Sample) %>%
  summarise("Average of Standardized Mean Differences" = round(mean(stat), 2),
            "Std. Deviation of Standardized Mean Differences" = round(sd(stat), 2)) %>%
  kable(align = c("l", "c"))
```

We can see that the average of standardized mean differences is 0.01 higher when using a 0.5 SD caliper than with a 1 SD caliper. We also display below the evolution of the difference in proportions for binary covariates:

```{r}
graph_love_plot_ps_2[["data"]] %>%
  filter(type == "Binary") %>%
  group_by(Sample) %>%
  summarise("Average of Standardized Mean Differences" = round(mean(stat), 2),
            "Std. Deviation of Standardized Mean Differences" = round(sd(stat), 2)) %>%
  kable(align = c("l", "c"))
```

Again, the stricter matching procedure did not help improve the balance of binary covariates.

We finally save the data on covariates balance in the `3.outputs/1.data/covariates_balance` folder.

```{r}
saveRDS(
  graph_love_plot_ps_2[["data"]],
  here::here(
    "3.outputs",
    "1.data",
    "covariates_balance",
    "data_cov_balance_ps.RDS"
  )
)
```


### Analysis of Matched Data

We now move to the analysis of the matched datasets using a simple regression model where we first regression the YoLL on the treatment indicator. We start with the matched data resulting from the propensity score with a 1 SD caliper. It is very important to note that the target causal estimand is not anymore the the average treatment on the treated as not all treated units could be matched to similar control units (2 treated units could not be matched). To know the true causal effect for the match data, we compute the mean difference in potential outcomes:

```{r}
# we retrieve the matched data
data_ps_1_caliper <- match.data(matching_ps_1_caliper)

# compute the true effect for the data
true_att_ps_1_caliper <-
  round(mean(data_ps_1_caliper$y_1[data_ps_1_caliper$heat_wave == 1] - data_ps_1_caliper$y_0[data_ps_1_caliper$heat_wave == 0]), 0)
```

The true effect for this match data is equal to +`r true_att_ps_1_caliper` YoLL, which is different for the true effect of 230 YoLL on **all** treated units. If we estimate this effect with a simple linear regression model, we get:

```{r}
# we fit the regression model
model_ps_1_caliper <- lm(y_obs ~ heat_wave,
                           data = data_ps_1_caliper,
                           weights = weights)

# retrieve the estimate and 95% ci
results_ps_1_caliper <- tidy(coeftest(
  model_ps_1_caliper,
  vcov. = vcovCL,
  cluster = ~ subclass
),
conf.int = TRUE) %>%
  filter(term == "heat_wave") %>%
  select(term, estimate, conf.low, conf.high) %>%
  mutate_at(vars(estimate:conf.high), ~ round(., 0))

# display results
results_ps_1_caliper %>%
  rename(
    "Term" = term,
    "Estimate" = estimate,
    "95% CI Lower Bound" = conf.low,
    "95% CI Upper Bound" = conf.high
  ) %>%
  kable(., align = c("l", "c", "c", "c"))
```

We find that the average effect on the treated is equal to +`r results_ps_1_caliper$estimate` years of life lost, which is the true causal effect. The 95% confidence interval is consistent with effects ranging from +`r results_ps_1_caliper$conf.low` up to +`r results_ps_1_caliper$conf.high`. If we want to increase the precision of our estimate and remove any remaining imbalance in covariates, we can also run a multivariate regression. We adjust below for the same variables used in the estimation of propensity scores:

```{r}
# we fit the regression model
model_ps_1_caliper_w_cov <-
  lm(
    y_obs ~ heat_wave + heat_wave_lag_1 + heat_wave_lag_2 + heat_wave_lag_3 + o3_lag_1 + o3_lag_2 + o3_lag_3 + no2 + no2_lag_1 + no2_lag_2 + no2_lag_3 + humidity_relative + month + week + year,
    data = data_ps_1_caliper,
    weights = weights
  )

# retrieve the estimate and 95% ci
results_ps_1_caliper_w_cov <- tidy(coeftest(
  model_ps_1_caliper_w_cov,
  vcov. = vcovCL,
  cluster = ~ subclass
),
conf.int = TRUE) %>%
  filter(term == "heat_wave") %>%
  select(term, estimate, conf.low, conf.high) %>%
  mutate_at(vars(estimate:conf.high), ~ round(., 0))

# display results
results_ps_1_caliper_w_cov %>%
  rename(
    "Term" = term,
    "Estimate" = estimate,
    "95% CI Lower Bound" = conf.low,
    "95% CI Upper Bound" = conf.high
  ) %>%
  kable(., align = c("l", "c", "c", "c"))
```


We find that the average effect on the treated is equal to +`r results_ps_1_caliper_w_cov$estimate` years of life lost: the estimate is now further away from the true effect. The 95% confidence interval is consistent with effects ranging from +`r results_ps_1_caliper_w_cov$conf.low` up to +`r results_ps_1_caliper_w_cov$conf.high`. The width of confidence interval is now equal to `r results_ps_1_caliper_w_cov$conf.high - results_ps_1_caliper_w_cov$conf.low`, which is about half smaller than the previous interval of `r results_ps_1_caliper$conf.high - results_ps_1_caliper$conf.low`.

We also estimate the treatment effect for the matched dataset resulting from the matching procedure with a 0.5 caliper. Again, it is very important to note that the target causal estimand is not anymore the the average treatment on the treated as not all treated units could be matched to similar control units: `r matching_ps_0.5_caliper[["nn"]][3]` treated units were matched. To know the true causal effect for the match data, we compute the mean difference in potential outcomes:

```{r}
# we retrieve the matched data
data_ps_0.5_caliper <- match.data(matching_ps_0.5_caliper)

# compute the true effect for the data
true_att_ps_0.5_caliper <-
  round(mean(data_ps_0.5_caliper$y_1[data_ps_0.5_caliper$heat_wave == 1] - data_ps_0.5_caliper$y_0[data_ps_0.5_caliper$heat_wave == 0]), 0)
```

The true effect for this match data is equal to +`r true_att_ps_0.5_caliper` YoLL, which is different for the true effect on **all** treated units and of the matched dataset for a 1 SD caliper. If we estimate this effect with a simple linear regression model, we get:

```{r}
# we fit the regression model
model_ps_0.5_caliper <- lm(y_obs ~ heat_wave,
                          data = data_ps_0.5_caliper,
                          weights = weights)

# retrieve the estimate and 95% ci
results_ps_0.5_caliper <- tidy(coeftest(
  model_ps_0.5_caliper,
  vcov. = vcovCL,
  cluster = ~ subclass
),
conf.int = TRUE) %>%
  filter(term == "heat_wave") %>%
  select(term, estimate, conf.low, conf.high) %>%
  mutate_at(vars(estimate:conf.high), ~ round(., 0))

# display results
results_ps_0.5_caliper %>%
  rename(
    "Term" = term,
    "Estimate" = estimate,
    "95% CI Lower Bound" = conf.low,
    "95% CI Upper Bound" = conf.high
  ) %>%
  kable(., align = c("l", "c", "c", "c"))
```

The estimated is exactly equal to the true effect of +`r true_att_ps_0.5_caliper`. The 95% confidence interval is consistent with effects ranging from +`r results_ps_0.5_caliper$conf.low` up to +`r results_ps_0.5_caliper$conf.high`. We finally run the regression model where we adjust for covariates:

```{r}
# we fit the regression model
model_ps_0.5_caliper_w_cov <-
  lm(
    y_obs ~ heat_wave + heat_wave_lag_1 + heat_wave_lag_2 + heat_wave_lag_3 + o3_lag_1 + o3_lag_2 + o3_lag_3 + no2 + no2_lag_1 + no2_lag_2 + no2_lag_3 + humidity_relative + month + week + year,
    data = data_ps_0.5_caliper,
    weights = weights
  )

# retrieve the estimate and 95% ci
results_ps_0.5_caliper_w_cov <- tidy(coeftest(
  model_ps_0.5_caliper_w_cov,
  vcov. = vcovCL,
  cluster = ~ subclass
),
conf.int = TRUE) %>%
  filter(term == "heat_wave") %>%
  select(term, estimate, conf.low, conf.high) %>%
  mutate_at(vars(estimate:conf.high), ~ round(., 0))

# display results
results_ps_0.5_caliper_w_cov %>%
  rename(
    "Term" = term,
    "Estimate" = estimate,
    "95% CI Lower Bound" = conf.low,
    "95% CI Upper Bound" = conf.high
  ) %>%
  kable(., align = c("l", "c", "c", "c"))
```

We find that the average effect on the treated is equal to +`r results_ps_0.5_caliper_w_cov$estimate` years of life lost: the estimate is still very close to the true effect. The 95% confidence interval is consistent with effects ranging from +`r results_ps_0.5_caliper_w_cov$conf.low` up to +`r results_ps_0.5_caliper_w_cov$conf.high`. The width of confidence interval is now equal to `r results_ps_0.5_caliper_w_cov$conf.high - results_ps_0.5_caliper_w_cov$conf.low`, which is absout half smaller than the previous interval of `r results_ps_0.5_caliper$conf.high - results_ps_0.5_caliper$conf.low`.


We finally save the data on results from propensity score analyses in the `3.outputs/1.data/analysis_results` folder.

```{r}
bind_rows(
  results_ps_1_caliper,
  results_ps_1_caliper_w_cov,
  results_ps_0.5_caliper,
  results_ps_0.5_caliper_w_cov
) %>%
  mutate(
    procedure = c(
      "Propensity Score with a 1 SD Caliper",
      "Propensity Score with a 1 SD Caliper and with Covariates Adjustment",
      "Propensity Score with a 0.5 SD Caliper",
      "Propensity Score with a 0.5 SD Caliper and with Covariates Adjustment"
    ),
    true_effect = c(rep(true_att_ps_1_caliper, 2), rep(true_att_ps_0.5_caliper, 2)),
    sample_size = c(rep(matching_ps_1_caliper[["nn"]][3]*2, 2), rep(matching_ps_0.5_caliper[["nn"]][3]*2, 2))
  ) %>%
  saveRDS(
    .,
    here::here(
      "3.outputs",
      "1.data",
      "analysis_results",
      "data_analysis_ps.RDS"
    )
  )
```